{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Proyecto 1 - Manuel Murguia\n",
        "### Deep Learning\n",
        "\n",
        "<p style = \"text-align: justify\"> En este proyecto, exploramos la generación de imágenes de paisajes utilizando autoencoders, redes neuronales capaces de comprimir y descomprimir datos. Nos centraremos en la arquitectura de autoencoder variacional (VAE), que no solo puede reconstruir imágenes, sino también generar nuevas muestras del espacio latente aprendido. Nuestro objetivo es crear paisajes realistas y estéticamente agradables, abordando desafíos como la diversidad y la calidad de las imágenes generadas."
      ],
      "metadata": {
        "id": "pDuRPBotclMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importación de Librerias"
      ],
      "metadata": {
        "id": "D5QObIoVbBgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import mean_squared_error\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ZUjPc6kj5sp2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conexión a Google Drive"
      ],
      "metadata": {
        "id": "AbJuI7oAbGAh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hljXJbzBFs3g",
        "outputId": "e8a73dfc-ad3d-4176-ca22-40fdfab970ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/DeepLearning/Proyecto1/archive.zip\"\n",
        "\n",
        "directory_path = \"/content/images\"\n",
        "\n",
        "# Crear el directorio de destino si no existe\n",
        "os.makedirs(directory_path, exist_ok=True)\n",
        "\n",
        "# Descomprimir el archivo ZIP\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_path)"
      ],
      "metadata": {
        "id": "7suD9ken5NN_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código no es nada especial, simplemente definimos las rutas dentro del google drive donde se encuentra el archivo zip con las imagenes y donde queremos depositarlas"
      ],
      "metadata": {
        "id": "zruTYtDPbJD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train y Test"
      ],
      "metadata": {
        "id": "R8T73-vZb4zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para cargar las imágenes\n",
        "def load_images_from_folder(folder, target_size = (32, 32)): #------------------------------------------------------------------------------- size en vez de 32\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = Image.open(os.path.join(folder, filename))\n",
        "        if img is not None:\n",
        "            # Redimensionar la imagen\n",
        "            img = img.resize(target_size)\n",
        "\n",
        "            # Convertir a formato RGB y asegurarse de que tenga solo 3 canales\n",
        "            img = img.convert('RGB')\n",
        "            images.append(np.array(img))\n",
        "\n",
        "            # Asigna una etiqueta (por ejemplo, 0 para entrenamiento, 1 para prueba)\n",
        "            labels.append(0 if random.random() < 0.8 else 1)\n",
        "    return images, labels\n",
        "\n",
        "# Directorio donde están tus imágenes\n",
        "root_directory = \"/content/images\"\n",
        "\n",
        "# Cargar las imágenes y las etiquetas\n",
        "images, labels = load_images_from_folder(root_directory)\n",
        "\n",
        "# Dividir las imágenes y las etiquetas en conjuntos de entrenamiento y prueba\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for img, label in zip(images, labels):\n",
        "    if label == 0:\n",
        "        x_train.append(img)\n",
        "        y_train.append(label)\n",
        "    else:\n",
        "        x_test.append(img)\n",
        "        y_test.append(label)\n",
        "\n",
        "# Convertir listas a numpy arrays\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Imprimir las dimensiones de los conjuntos de entrenamiento y prueba\n",
        "print('Dimensiones de x_train:', x_train.shape)\n",
        "print('Dimensiones de x_test:', x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "j7buZ33Y8iIV",
        "outputId": "9469cf2f-3cf3-4901-bbcf-aba4f523bee7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones de x_train: (3448, 32, 32, 3)\n",
            "Dimensiones de y_train: (3448,)\n",
            "Dimensiones de x_test: (871, 32, 32, 3)\n",
            "Dimensiones de y_test: (871,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código anterior nos separa nuestro dataset en train y test. El train lo usaremos para entrenar nuestro modelo mientras que el test servirá para comprobar la funcionalidad de nuestro modelo."
      ],
      "metadata": {
        "id": "Y0c_RdmRbdDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imagen de Ejemplo"
      ],
      "metadata": {
        "id": "3B_KudJ3cIGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir el lote de imágenes a un arreglo NumPy\n",
        "images = x_train\n",
        "\n",
        "# Seleccionar una imagen aleatoria del lote\n",
        "index = np.random.randint(len(images))\n",
        "\n",
        "image = images[0]\n",
        "\n",
        "# Mostrar la imagen\n",
        "plt.figure(figsize = (5, 5))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "pVwqOoIG86oA",
        "outputId": "dbaa980a-9cea-43ba-ec4a-47ec4451f74e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí podemos ver como se vería una imagen aleatoria del dataset de entrenamiento"
      ],
      "metadata": {
        "id": "2wdHdPC1btP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape[1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pxVF6w4YDIIr",
        "outputId": "a16720eb-458f-4d98-c835-0ad37d7c152d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y sus dimensiones"
      ],
      "metadata": {
        "id": "W7qCVfNobzz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "metadata": {
        "id": "y13StJESAAJH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalizamos los pixeles de las imágenes"
      ],
      "metadata": {
        "id": "SKrG5VIZcTaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "batch_size = 64\n",
        "latent_dim = 64\n",
        "epochs = 15"
      ],
      "metadata": {
        "id": "tSxYUDJH6Y2c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "y definimos nuestros parámetros. Cabe mencionar que estos parámetros fueron definidos déspues de varias pruebas, no los obtuve de manera aleatoria, simplemente fueron los que mejor resultado me dieron."
      ],
      "metadata": {
        "id": "ZMsbHOKScWVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape = input_shape, name = \"encoder_input\")\n",
        "x = Conv2D(32, 3, activation = \"relu\", strides = 2, padding = \"same\")(inputs)\n",
        "x = Conv2D(64, 3, activation = \"relu\", strides = 2, padding =  \"same\")(x)\n",
        "\n",
        "shape_before_flat = K.int_shape(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation = \"relu\" )(x)\n",
        "\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)"
      ],
      "metadata": {
        "id": "2MqwKV8ePJoh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código anterior define la parte del \"encoder\" de un autoencoder variacional, que toma una entrada de imagen, la procesa a través de capas convolucionales y capas densas, y produce la media y la varianza de la distribución latente."
      ],
      "metadata": {
        "id": "xY6yrYfVdJJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling(args):\n",
        "  z_mean, z_log_var = args\n",
        "\n",
        "  dim = K.int_shape(z_mean)[1]\n",
        "\n",
        "  # TODO: check dimensions\n",
        "  epsilon = K.random_normal(shape = (K.shape(z_mean)[0], dim))\n",
        "\n",
        "  return z_mean + K.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "xVphTuriPSvN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style = \"text-align: justify\"> El código define una función llamada sampling para un autoencoder especial llamado autoencoder variacional (VAE). Esta función es como la imaginación del modelo: toma una idea general (la media y la varianza) y la convierte en algo específico y nuevo. Utiliza una especie de truco matemático para agregar un poco de variabilidad a las muestras que genera, lo que le da al modelo la capacidad de crear imágenes diferentes cada vez que lo usamos. Es como si estuviera añadiendo un poco de especias a una receta básica para hacerla más emocionante y variada. </p>"
      ],
      "metadata": {
        "id": "vytWNUFSejU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])"
      ],
      "metadata": {
        "id": "YZEHz6OGPU1z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "esta línea de código crea una capa en el modelo VAE que toma la media y el logaritmo de la varianza de la distribución latente como entrada y genera muestras en el espacio latente utilizando la función sampling."
      ],
      "metadata": {
        "id": "fJwjSJ8JfDYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "kXiTCWY6MFKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1rmyhpn2MBF6",
        "outputId": "0d3123b6-9ca7-44c7-a7b3-45490e6c3176"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)  [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 16, 16, 32)           896       ['encoder_input[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 4096)                 0         ['conv2d_1[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 256)                  1048832   ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " z_mean (Dense)              (None, 64)                   16448     ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " z_log_var (Dense)           (None, 64)                   16448     ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " z (Lambda)                  (None, 64)                   0         ['z_mean[0][0]',              \n",
            "                                                                     'z_log_var[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1101120 (4.20 MB)\n",
            "Trainable params: 1101120 (4.20 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "este fragmento de código crea y muestra un resumen del modelo encoder, que toma una entrada y produce la media (z_mean) y el logaritmo de la varianza (z_log_var) de la distribución latente, así como las muestras (z) en el espacio latente, utilizando la función de muestreo definida anteriormente."
      ],
      "metadata": {
        "id": "h0IV_-25fGqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "\n",
        "# Check if work\n",
        "x = Dense(np.prod(shape_before_flat[1:]), activation = \"relu\")(latent_inputs)\n",
        "x = Reshape(shape_before_flat[1:])(x)\n",
        "x = Conv2DTranspose(64, 3, activation = \"relu\", strides = 2, padding =  \"same\")(x)\n",
        "x = Conv2DTranspose(32, 3, activation = \"relu\", strides = 2, padding = \"same\")(x)\n",
        "outputs = Conv2DTranspose(3, 3, activation = \"sigmoid\", padding = \"same\")(x)\n",
        "\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AthQRd5CPby9",
        "outputId": "3eb82dce-3e7f-42f3-e8f3-9fff33ce2e00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " z_sampling (InputLayer)     [(None, 64)]              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              266240    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTr  (None, 16, 16, 64)        36928     \n",
            " anspose)                                                        \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2D  (None, 32, 32, 32)        18464     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2D  (None, 32, 32, 3)         867       \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 322499 (1.23 MB)\n",
            "Trainable params: 322499 (1.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "este fragmento de código define el decodificador del modelo VAE, que toma muestras del espacio latente como entrada y produce imágenes reconstruidas como salida, utilizando capas de convolución transpuesta para deshacer las transformaciones realizadas por el codificador."
      ],
      "metadata": {
        "id": "mdbiFdr6fi9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae')"
      ],
      "metadata": {
        "id": "qcvaemeDPeUd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "este fragmento de código crea el modelo completo del autoencoder variacional tomando las imagenes como entrada y pasandolas por el codificador"
      ],
      "metadata": {
        "id": "CVmgvpM3f1Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_loss = mean_squared_error(K.flatten(inputs), K.flatten(outputs)) * input_shape[0] * input_shape[1]\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)"
      ],
      "metadata": {
        "id": "GFDBrrK6PgK5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "este bloque de código calcula la función de pérdida del modelo de autoencoder variacional (VAE), que consta de dos partes: la pérdida de reconstrucción y la pérdida de regularización de la divergencia de Kullback-Leibler"
      ],
      "metadata": {
        "id": "hzacqjg0f-Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "vae.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Mb_XXRScPhw1",
        "outputId": "b19e66b2-c688-4c15-b886-6e7657f1d001"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vae\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)  [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " encoder (Functional)        [(None, 64),                 1101120   ['encoder_input[0][0]']       \n",
            "                              (None, 64),                                                         \n",
            "                              (None, 64)]                                                         \n",
            "                                                                                                  \n",
            " decoder (Functional)        (None, 32, 32, 3)            322499    ['encoder[0][2]']             \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 16, 16, 32)           896       ['encoder_input[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 4096)                 0         ['conv2d_1[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 256)                  1048832   ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " tf.reshape_1 (TFOpLambda)   (None,)                      0         ['decoder[0][0]']             \n",
            "                                                                                                  \n",
            " tf.reshape (TFOpLambda)     (None,)                      0         ['encoder_input[0][0]']       \n",
            "                                                                                                  \n",
            " z_log_var (Dense)           (None, 64)                   16448     ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " z_mean (Dense)              (None, 64)                   16448     ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor (TFOp  (None,)                      0         ['tf.reshape_1[0][0]']        \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.cast (TFOpLambda)        (None,)                      0         ['tf.reshape[0][0]']          \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 64)                   0         ['z_log_var[0][0]']           \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.math.square (TFOpLambda  (None, 64)                   0         ['z_mean[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.squared_difference  (None,)                      0         ['tf.convert_to_tensor[0][0]',\n",
            "  (TFOpLambda)                                                       'tf.cast[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.subtract (TFOpLamb  (None, 64)                   0         ['tf.__operators__.add[0][0]',\n",
            " da)                                                                 'tf.math.square[0][0]']      \n",
            "                                                                                                  \n",
            " tf.math.exp (TFOpLambda)    (None, 64)                   0         ['z_log_var[0][0]']           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpL  ()                           0         ['tf.math.squared_difference[0\n",
            " ambda)                                                             ][0]']                        \n",
            "                                                                                                  \n",
            " tf.math.subtract_1 (TFOpLa  (None, 64)                   0         ['tf.math.subtract[0][0]',    \n",
            " mbda)                                                               'tf.math.exp[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLamb  ()                           0         ['tf.math.reduce_mean[0][0]'] \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum (TFOpLa  (None,)                      0         ['tf.math.subtract_1[0][0]']  \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.math.multiply_1 (TFOpLa  ()                           0         ['tf.math.multiply[0][0]']    \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.math.multiply_2 (TFOpLa  (None,)                      0         ['tf.math.reduce_sum[0][0]']  \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TF  (None,)                      0         ['tf.math.multiply_1[0][0]',  \n",
            " OpLambda)                                                           'tf.math.multiply_2[0][0]']  \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_1 (TFO  ()                           0         ['tf.__operators__.add_1[0][0]\n",
            " pLambda)                                                           ']                            \n",
            "                                                                                                  \n",
            " add_loss (AddLoss)          ()                           0         ['tf.math.reduce_mean_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1423619 (5.43 MB)\n",
            "Trainable params: 1423619 (5.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "este bloque de código completa la configuración del modelo VAE al agregar la función de pérdida al modelo, compilarlo con un optimizador y mostrar un resumen del modelo"
      ],
      "metadata": {
        "id": "PJDxX7NlgGOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrenamos"
      ],
      "metadata": {
        "id": "RJ2wtbj2foiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the autoencoder\n",
        "vae.fit(x_train,\n",
        "        epochs = epochs,\n",
        "        batch_size = batch_size,\n",
        "        validation_data = (x_test, None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WBnWZD8xPk03",
        "outputId": "f97b68c9-1711-440c-aa91-e9fa13120de2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "54/54 [==============================] - 16s 254ms/step - loss: 66.2592 - val_loss: 58.4206\n",
            "Epoch 2/15\n",
            "54/54 [==============================] - 13s 245ms/step - loss: 55.8245 - val_loss: 52.8351\n",
            "Epoch 3/15\n",
            "54/54 [==============================] - 14s 259ms/step - loss: 47.5117 - val_loss: 45.6041\n",
            "Epoch 4/15\n",
            "54/54 [==============================] - 13s 243ms/step - loss: 43.1826 - val_loss: 43.7994\n",
            "Epoch 5/15\n",
            "54/54 [==============================] - 13s 241ms/step - loss: 42.0078 - val_loss: 41.9615\n",
            "Epoch 6/15\n",
            "54/54 [==============================] - 16s 303ms/step - loss: 40.6912 - val_loss: 40.9282\n",
            "Epoch 7/15\n",
            "54/54 [==============================] - 10s 190ms/step - loss: 39.7933 - val_loss: 40.0253\n",
            "Epoch 8/15\n",
            "54/54 [==============================] - 12s 222ms/step - loss: 38.6001 - val_loss: 39.4212\n",
            "Epoch 9/15\n",
            "54/54 [==============================] - 12s 233ms/step - loss: 37.8917 - val_loss: 38.3417\n",
            "Epoch 10/15\n",
            "54/54 [==============================] - 11s 199ms/step - loss: 36.8559 - val_loss: 37.7969\n",
            "Epoch 11/15\n",
            "54/54 [==============================] - 12s 216ms/step - loss: 36.0629 - val_loss: 36.7659\n",
            "Epoch 12/15\n",
            "54/54 [==============================] - 13s 236ms/step - loss: 35.3775 - val_loss: 36.1062\n",
            "Epoch 13/15\n",
            "54/54 [==============================] - 11s 205ms/step - loss: 35.0706 - val_loss: 35.7052\n",
            "Epoch 14/15\n",
            "54/54 [==============================] - 11s 202ms/step - loss: 34.5119 - val_loss: 35.1166\n",
            "Epoch 15/15\n",
            "54/54 [==============================] - 12s 232ms/step - loss: 34.1298 - val_loss: 34.6419\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c2a9adb65f0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código realiza el entrenamiento del autoencoder variacional (VAE) utilizando los datos de entrenamiento x_train"
      ],
      "metadata": {
        "id": "FscjtwjPgKoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener z_mean, z_log_var para un lote de imágenes de entrada\n",
        "z_mean_batch, z_log_var_batch, _ = encoder.predict(x_test)\n",
        "\n",
        "# Utilizar la función de muestreo para generar muestras de la distribución latente\n",
        "latent_samples = sampling([z_mean_batch, z_log_var_batch])\n",
        "\n",
        "# Decodificar las muestras generadas para obtener imágenes \"inventadas\"\n",
        "decoded_images = decoder.predict(latent_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "86mhjXxePrmQ",
        "outputId": "462e1ea0-5e30-4b2d-dbad-7e96ea8760c9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28/28 [==============================] - 0s 8ms/step\n",
            "28/28 [==============================] - 1s 22ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Después de entrenar el modelo VAE, el código utiliza el codificador para obtener información sobre cómo se distribuyen las imágenes de prueba en el espacio latente. Luego, genera nuevas muestras en este espacio latente utilizando estas distribuciones aprendidas. Estas muestras se decodifican para generar imágenes \"inventadas\", que son versiones creativas de las originales, explorando diferentes aspectos del espacio latente y generando nuevas imágenes basadas en lo que el modelo ha aprendido durante el entrenamiento."
      ],
      "metadata": {
        "id": "OOsgppKQglfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imágenes Generadas"
      ],
      "metadata": {
        "id": "dIgPURtgfsAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegir una de las imágenes generadas para mostrar\n",
        "image_to_show = decoded_images[0]\n",
        "\n",
        "# Mostrar la imagen generada\n",
        "plt.imshow(image_to_show)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "Mr96rNzRPv4M",
        "outputId": "c34136ad-c696-491f-efea-ea48d4f1b738"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque de código elige una de las imágenes generadas por el modelo y la muestra visualmente. Si bien es cierto que no se distingue con claridad que hay en la imagen generada, podemos observar detalles claros como el cielo, nubes, y un posible bosque o montañas en la parte inferior de la imagen."
      ],
      "metadata": {
        "id": "acchuU4_grNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(decoded_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ksh8khtXYRiv",
        "outputId": "98b2b459-d4e3-4c0a-fb2d-a368f8312d8a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "871"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}